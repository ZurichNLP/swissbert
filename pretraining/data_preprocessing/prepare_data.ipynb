{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from swissdox import SwissdoxData\n",
    "import tokenization\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = [\n",
    "    \"de_CH\",\n",
    "    \"fr_CH\",\n",
    "    \"it_CH\",\n",
    "    \"rm_CH\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/swissdox\")\n",
    "assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_xlm_vocab = data_dir / \"xlm_vocab\"\n",
    "out_dir_xlm_vocab.mkdir(exist_ok=True)\n",
    "out_dir_new_vocab = data_dir / \"new_vocab\"\n",
    "out_dir_new_vocab.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract articles from the Swissdox@LiRI TSV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for language in LANGUAGES:\n",
    "    print(language)\n",
    "    data = SwissdoxData(data_dir / \"raw\" / f\"{language}.tsv\")\n",
    "    articles = list(data.get_articles())\n",
    "    train, valid = utils.create_split(articles)\n",
    "    print(f\"train: {len(train)} articles\")\n",
    "    print(f\"valid: {len(valid)} articles\")\n",
    "\n",
    "    print(\"XLM vocab => no custom special tokens\")\n",
    "    add_metadata = True\n",
    "    metadata_use_special_tokens = False\n",
    "    train_path = out_dir_xlm_vocab / f\"{language}.train.txt\"\n",
    "    with open(train_path, \"w\") as f:\n",
    "        for article in train:\n",
    "            article = article.to_txt(add_metadata=add_metadata, metadata_use_special_tokens=metadata_use_special_tokens)\n",
    "            f.write(article + \"\\n\\n\")\n",
    "    valid_path = out_dir_xlm_vocab / f\"{language}.valid.txt\"\n",
    "    with open(valid_path, \"w\") as f:\n",
    "        for article in valid:\n",
    "            article = article.to_txt(add_metadata=add_metadata, metadata_use_special_tokens=metadata_use_special_tokens)\n",
    "            f.write(article + \"\\n\\n\")\n",
    "\n",
    "    print(\"Custom vocab => custom special tokens\")\n",
    "    add_metadata = True\n",
    "    metadata_use_special_tokens = True\n",
    "    train_path = out_dir_new_vocab / f\"{language}.train.txt\"\n",
    "    with open(train_path, \"w\") as f:\n",
    "        for article in train:\n",
    "            article = article.to_txt(add_metadata=add_metadata, metadata_use_special_tokens=metadata_use_special_tokens)\n",
    "            f.write(article + \"\\n\\n\")\n",
    "    valid_path = out_dir_new_vocab / f\"{language}.valid.txt\"\n",
    "    with open(valid_path, \"w\") as f:\n",
    "        for article in valid:\n",
    "            article = article.to_txt(add_metadata=add_metadata, metadata_use_special_tokens=metadata_use_special_tokens)\n",
    "            f.write(article + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with XLM vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for language in LANGUAGES:\n",
    "    for txt_path in [\n",
    "        out_dir_xlm_vocab / f\"{language}.train.txt\",\n",
    "        out_dir_xlm_vocab / f\"{language}.valid.txt\"\n",
    "    ]:\n",
    "        tokenization.tokenize_xlm(\n",
    "            txt_path,\n",
    "            txt_path.with_suffix(f\".xlm.bpe\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with new vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization.create_spm_vocabulary(\n",
    "    txt_paths=[out_dir_new_vocab / f\"{language}.train.txt\" for language in LANGUAGES],\n",
    "    name=\"swissbert\",\n",
    "    sampling_alpha=0.3,\n",
    "    vocab_size=50260,\n",
    "    user_defined_symbols=[\"</s>\", \"<medium>\", \"<year>\", \"<month>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"swissbert.v1.model\")\n",
    "vocab_path = Path(\"swissbert.v1.vocab\")\n",
    "assert model_path.exists()\n",
    "assert vocab_path.exists()\n",
    "vocab_dir = Path(\"../vocab\")\n",
    "assert vocab_dir.exists()\n",
    "shutil.move(model_path, vocab_dir / model_path.name)\n",
    "shutil.move(vocab_path, vocab_dir / vocab_path.name)\n",
    "swissbert_model_path = vocab_dir / model_path.name\n",
    "swissbert_vocab_path = vocab_dir / vocab_path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in LANGUAGES:\n",
    "    for txt_path in [\n",
    "        out_dir_new_vocab / f\"{language}.train.txt\",\n",
    "        out_dir_new_vocab / f\"{language}.valid.txt\"\n",
    "    ]:\n",
    "        tokenization.tokenize_hf(\n",
    "            swissbert_model_path,\n",
    "            txt_path,\n",
    "            txt_path.with_suffix(f\".new.bpe\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"DATA_DIR\"] = str(out_dir_xlm_vocab.resolve())\n",
    "for language in LANGUAGES:\n",
    "    os.environ[\"LANGUAGE\"] = language\n",
    "    !fairseq-preprocess \\\n",
    "      --only-source \\\n",
    "      --trainpref \"$DATA_DIR/$LANGUAGE.train.xlm.bpe\" \\\n",
    "      --validpref \"$DATA_DIR/$LANGUAGE.valid.xlm.bpe\" \\\n",
    "      --destdir \"$DATA_DIR/bin/$LANGUAGE\" \\\n",
    "      --bpe sentencepiece \\\n",
    "      --srcdict ../vocab/xlm.dict.txt \\\n",
    "      --workers 20\n",
    "    !rm \"$DATA_DIR/bin/$LANGUAGE/dict.txt\"\n",
    "    !cp ../vocab/xlm.dict.txt \"$DATA_DIR/bin/dict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spm vocab to fairseq format\n",
    "swissbert_dict_path = swissbert_vocab_path.with_suffix(\".dict.txt\")\n",
    "with open(swissbert_vocab_path) as f_in, open(swissbert_dict_path, \"w\") as f_out:\n",
    "    for line in f_in:\n",
    "        token, _ = line.split()\n",
    "        if token in {\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"}:\n",
    "            continue\n",
    "        f_out.write(f\"{token} 1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_DIR\"] = str(out_dir_new_vocab.resolve())\n",
    "os.environ[\"DICT_PATH\"] = str(swissbert_dict_path.resolve())\n",
    "for language in LANGUAGES:\n",
    "    os.environ[\"LANGUAGE\"] = language\n",
    "    !fairseq-preprocess \\\n",
    "      --only-source \\\n",
    "      --trainpref \"$DATA_DIR/$LANGUAGE.train.new.bpe\" \\\n",
    "      --validpref \"$DATA_DIR/$LANGUAGE.valid.new.bpe\" \\\n",
    "      --destdir \"$DATA_DIR/bin/$LANGUAGE\" \\\n",
    "      --bpe sentencepiece \\\n",
    "      --srcdict \"$DICT_PATH\" \\\n",
    "      --workers 20\n",
    "    !rm \"$DATA_DIR/bin/$LANGUAGE/dict.txt\"\n",
    "    !cp $DICT_PATH \"$DATA_DIR/bin/dict.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xmod",
   "language": "python",
   "name": "xmod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
