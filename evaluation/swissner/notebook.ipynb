{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "xfDGuY9p2SZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgAf0c5dyeUF",
        "outputId": "ab69965d-4aec-4cf4-d481-e05e37ff5ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.27.1\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.0\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines==3.1.0\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (4.65.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (1.4.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (2023.3.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines==3.1.0->-r requirements.txt (line 6)) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 5)) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=a02dd62b8c325812595853d9e8bf2bc3c4dece58695d6ce943826ed80001dd21\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tokenizers, sentencepiece, xxhash, multidict, jsonlines, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, seqeval, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.13.3 jsonlines-3.1.0 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 sentencepiece-0.1.97 seqeval-1.2.2 tokenizers-0.13.2 transformers-4.27.1 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "nNz9gFQwy58n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_script_path = Path(\"run_ner.py\")\n",
        "assert ner_script_path.exists()"
      ],
      "metadata": {
        "id": "KSe6aVmrzM8Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ZurichNLP/swissbert\""
      ],
      "metadata": {
        "id": "_d7NB6qvzHDQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = Path(\"finetuned_models\") / model_name\n",
        "model_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "9q5o8qNXzLk6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on WikiNEuRal dataset"
      ],
      "metadata": {
        "id": "FHqAUBp32OuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set num_train_epochs to 3 to reproduce paper\n",
        "!python run_ner.py \\\n",
        "  --seed 553589 \\\n",
        "  --model_name_or_path {model_name} \\\n",
        "  --dataset_name wikineural_defrit \\\n",
        "  --output_dir {model_dir.resolve()} \\\n",
        "  --overwrite_output_dir \\\n",
        "  --save_strategy no \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 0.25 \\\n",
        "  --do_eval \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --per_device_train_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv4Ipgud0wxg",
        "outputId": "4458b21a-cb99-4584-bbcb-4d76fd08b839"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-20 17:28:13.695377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-20 17:28:14.776972: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-20 17:28:14.777080: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-20 17:28:14.777101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "03/20/2023 17:28:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading readme: 100% 5.27k/5.27k [00:00<00:00, 5.16MB/s]\n",
            "Downloading and preparing dataset parquet/Babelscape--wikineural to /root/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
            "Downloading data files:   0% 0/27 [00:00<?, ?it/s]\n",
            "Downloading data: 100% 993k/993k [00:00<00:00, 19.6MB/s]\n",
            "Downloading data files:   4% 1/27 [00:00<00:13,  1.94it/s]\n",
            "Downloading data:   0% 0.00/7.52M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 7.52M/7.52M [00:00<00:00, 59.8MB/s]\n",
            "Downloading data files:   7% 2/27 [00:01<00:13,  1.82it/s]\n",
            "Downloading data: 100% 1.04M/1.04M [00:00<00:00, 19.7MB/s]\n",
            "Downloading data files:  11% 3/27 [00:01<00:12,  1.98it/s]\n",
            "Downloading data:   0% 0.00/7.81M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 7.81M/7.81M [00:00<00:00, 47.8MB/s]\n",
            "Downloading data files:  15% 4/27 [00:02<00:12,  1.88it/s]\n",
            "Downloading data:   0% 0.00/6.43M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 6.43M/6.43M [00:00<00:00, 46.6MB/s]\n",
            "Downloading data files:  19% 5/27 [00:02<00:11,  1.86it/s]\n",
            "Downloading data:   0% 0.00/7.18M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 7.18M/7.18M [00:00<00:00, 44.9MB/s]\n",
            "Downloading data files:  22% 6/27 [00:03<00:11,  1.84it/s]\n",
            "Downloading data: 100% 853k/853k [00:00<00:00, 15.5MB/s]\n",
            "Downloading data files:  26% 7/27 [00:03<00:10,  1.90it/s]\n",
            "Downloading data:   0% 0.00/9.12M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 9.12M/9.12M [00:00<00:00, 64.3MB/s]\n",
            "Downloading data files:  30% 8/27 [00:04<00:10,  1.89it/s]\n",
            "Downloading data:   0% 0.00/5.31M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 5.31M/5.31M [00:00<00:00, 48.6MB/s]\n",
            "Downloading data files:  33% 9/27 [00:04<00:09,  1.92it/s]\n",
            "Downloading data: 100% 706k/706k [00:00<00:00, 14.7MB/s]\n",
            "Downloading data files:  37% 10/27 [00:05<00:08,  2.04it/s]\n",
            "Downloading data:   0% 0.00/7.05M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 7.05M/7.05M [00:00<00:00, 63.6MB/s]\n",
            "Downloading data files:  41% 11/27 [00:05<00:07,  2.03it/s]\n",
            "Downloading data: 100% 723k/723k [00:00<00:00, 15.1MB/s]\n",
            "Downloading data files:  44% 12/27 [00:06<00:07,  2.11it/s]\n",
            "Downloading data: 100% 702k/702k [00:00<00:00, 13.6MB/s]\n",
            "Downloading data files:  48% 13/27 [00:06<00:06,  2.17it/s]\n",
            "Downloading data: 100% 849k/849k [00:00<00:00, 16.8MB/s]\n",
            "Downloading data files:  52% 14/27 [00:06<00:05,  2.19it/s]\n",
            "Downloading data: 100% 894k/894k [00:00<00:00, 16.4MB/s]\n",
            "Downloading data files:  56% 15/27 [00:07<00:05,  2.23it/s]\n",
            "Downloading data: 100% 711k/711k [00:00<00:00, 14.6MB/s]\n",
            "Downloading data files:  59% 16/27 [00:07<00:04,  2.22it/s]\n",
            "Downloading data: 100% 841k/841k [00:00<00:00, 16.5MB/s]\n",
            "Downloading data files:  63% 17/27 [00:08<00:04,  2.22it/s]\n",
            "Downloading data:   0% 0.00/6.04M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 6.04M/6.04M [00:00<00:00, 54.1MB/s]\n",
            "Downloading data files:  67% 18/27 [00:08<00:04,  2.17it/s]\n",
            "Downloading data: 100% 899k/899k [00:00<00:00, 17.6MB/s]\n",
            "Downloading data files:  70% 19/27 [00:09<00:03,  2.21it/s]\n",
            "Downloading data: 100% 847k/847k [00:00<00:00, 16.8MB/s]\n",
            "Downloading data files:  74% 20/27 [00:09<00:03,  2.25it/s]\n",
            "Downloading data: 100% 1.04M/1.04M [00:00<00:00, 19.0MB/s]\n",
            "Downloading data files:  78% 21/27 [00:10<00:02,  2.24it/s]\n",
            "Downloading data: 100% 1.06M/1.06M [00:00<00:00, 17.2MB/s]\n",
            "Downloading data files:  81% 22/27 [00:10<00:02,  2.20it/s]\n",
            "Downloading data: 100% 1.11M/1.11M [00:00<00:00, 20.8MB/s]\n",
            "Downloading data files:  85% 23/27 [00:11<00:01,  2.23it/s]\n",
            "Downloading data: 100% 1.12M/1.12M [00:00<00:00, 20.5MB/s]\n",
            "Downloading data files:  89% 24/27 [00:11<00:01,  2.22it/s]\n",
            "Downloading data: 100% 997k/997k [00:00<00:00, 18.8MB/s]\n",
            "Downloading data files:  93% 25/27 [00:11<00:00,  2.23it/s]\n",
            "Downloading data:   0% 0.00/8.31M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 8.31M/8.31M [00:00<00:00, 64.2MB/s]\n",
            "Downloading data files:  96% 26/27 [00:12<00:00,  2.14it/s]\n",
            "Downloading data: 100% 943k/943k [00:00<00:00, 18.7MB/s]\n",
            "Downloading data files: 100% 27/27 [00:12<00:00,  2.10it/s]\n",
            "Extracting data files: 100% 27/27 [00:00<00:00, 2353.07it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
            "100% 27/27 [00:00<00:00, 322.45it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 854/854 [00:00<00:00, 117kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 403/403 [00:00<00:00, 71.5kB/s]\n",
            "Downloading (…)tencepiece.bpe.model: 100% 1.16M/1.16M [00:00<00:00, 21.0MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 1.18k/1.18k [00:00<00:00, 489kB/s]\n",
            "Downloading pytorch_model.bin: 100% 612M/612M [00:04<00:00, 125MB/s]\n",
            "[WARNING|modeling_utils.py:3022] 2023-03-20 17:30:26,267 >> Some weights of the model checkpoint at ZurichNLP/swissbert were not used when initializing XmodForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XmodForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XmodForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3034] 2023-03-20 17:30:26,267 >> Some weights of XmodForTokenClassification were not initialized from the model checkpoint at ZurichNLP/swissbert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 5.43MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0% 0/4498 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-03-20 17:31:53,751 >> You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.2463, 'learning_rate': 1.7776789684304137e-05, 'epoch': 0.03}\n",
            "{'loss': 0.076, 'learning_rate': 1.555357936860827e-05, 'epoch': 0.06}\n",
            "{'loss': 0.0594, 'learning_rate': 1.3330369052912406e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0511, 'learning_rate': 1.1107158737216541e-05, 'epoch': 0.11}\n",
            "{'loss': 0.046, 'learning_rate': 8.883948421520675e-06, 'epoch': 0.14}\n",
            "{'loss': 0.0433, 'learning_rate': 6.660738105824811e-06, 'epoch': 0.17}\n",
            "{'loss': 0.041, 'learning_rate': 4.437527790128947e-06, 'epoch': 0.19}\n",
            "{'loss': 0.0365, 'learning_rate': 2.2143174744330814e-06, 'epoch': 0.22}\n",
            "{'train_runtime': 1234.9569, 'train_samples_per_second': 58.269, 'train_steps_per_second': 3.642, 'train_loss': 0.07060579195612003, 'epoch': 0.25}\n",
            "100% 4498/4498 [20:34<00:00,  3.64it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =       0.25\n",
            "  train_loss               =     0.0706\n",
            "  train_runtime            = 0:20:34.95\n",
            "  train_samples            =     287840\n",
            "  train_samples_per_second =     58.269\n",
            "  train_steps_per_second   =      3.642\n",
            "100% 4498/4498 [03:07<00:00, 23.98it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       0.25\n",
            "  eval_accuracy           =     0.9903\n",
            "  eval_f1                 =     0.9282\n",
            "  eval_loss               =     0.0345\n",
            "  eval_precision          =     0.9241\n",
            "  eval_recall             =     0.9323\n",
            "  eval_runtime            = 0:03:07.61\n",
            "  eval_samples            =      35980\n",
            "  eval_samples_per_second =    191.774\n",
            "  eval_steps_per_second   =     23.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on SwissNER"
      ],
      "metadata": {
        "id": "B9PRqHQM2cnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language = \"de\"\n",
        "# language = \"fr\"\n",
        "# language = \"it\"\n",
        "# language = \"rm\"\n",
        "\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path {model_dir.resolve()} \\\n",
        "  --dataset_name swissner_{language} \\\n",
        "  --dataset_config_name {language} \\\n",
        "  --output_dir {(model_dir / (\"swissner_\" + language)).resolve()} \\\n",
        "  --overwrite_output_dir \\\n",
        "  --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9ahBzXg0QW-",
        "outputId": "0d08c5da-258a-455e-fb2a-58a245984cc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-20 17:58:43.984037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-20 17:58:44.945844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-20 17:58:44.945966: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-20 17:58:44.945988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "03/20/2023 17:58:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/20/2023 17:58:48 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/ZurichNLP___parquet/ZurichNLP--swissner-0f979f8852e01f16/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100% 4/4 [00:00<00:00, 885.62it/s]\n",
            "[WARNING|logging.py:280] 2023-03-20 17:58:53,595 >> You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 25/25 [00:01<00:00, 19.12it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9727\n",
            "  predict_f1                 =     0.7608\n",
            "  predict_loss               =     0.1147\n",
            "  predict_precision          =     0.8596\n",
            "  predict_recall             =     0.6824\n",
            "  predict_runtime            = 0:00:01.93\n",
            "  predict_samples_per_second =    103.311\n",
            "  predict_steps_per_second   =     12.914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLTxPKQlLGJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}